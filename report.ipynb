{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset1</td>\n",
       "      <td>0.085024</td>\n",
       "      <td>0.150851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset1_all</td>\n",
       "      <td>0.083356</td>\n",
       "      <td>0.146663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset2</td>\n",
       "      <td>0.347268</td>\n",
       "      <td>0.514941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset2_all</td>\n",
       "      <td>0.346969</td>\n",
       "      <td>0.514539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset3</td>\n",
       "      <td>0.081689</td>\n",
       "      <td>0.139547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dataset3_all</td>\n",
       "      <td>0.071687</td>\n",
       "      <td>0.115201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dataset  Accuracy  F1 Score\n",
       "0      dataset1  0.085024  0.150851\n",
       "1  dataset1_all  0.083356  0.146663\n",
       "2      dataset2  0.347268  0.514941\n",
       "3  dataset2_all  0.346969  0.514539\n",
       "4      dataset3  0.081689  0.139547\n",
       "5  dataset3_all  0.071687  0.115201"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score,f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#데이터 받기\n",
    "folder_path = \"C:/Users/milab_8/Desktop/내꺼/BGP_RIPE_datasets_for_anomaly_detection_csv_revised_19022021\"\n",
    "BCNET = pd.read_csv(folder_path +'/BCNET_regular.csv')\n",
    "RED1=pd.read_csv(folder_path+'/Code_Red_I.csv')\n",
    "Nimda=pd.read_csv(folder_path+'/Nimda.csv')\n",
    "RIPE=pd.read_csv(folder_path+'/RIPE_regular.csv')\n",
    "Slammer=pd.read_csv(folder_path+'/Slammer.csv')\n",
    "\n",
    "\n",
    "\n",
    "#열이름 붙여주는 함수\n",
    "def plus_col(file):\n",
    "    col_name=file.columns\n",
    "    new_col_name=['hour_minute','hour','minute','second','Number of announcements','Number of withdrawals','Number of announced NLRI prefixes','Number of withdrawn NLRI prefixes','Average AS-path length','Maximum AS-path length',\n",
    "                  'Average unique AS-path length',\"Number of duplicate announcements\",'Number of implicit withdrawals','Number of duplicate withdrawals','Maximum edit distance','Arrival rate','Average edit distance',\n",
    "                  'Maximum AS-path length 11','Maximum AS-path length 12','Maximum AS-path length 13','Maximum AS-path length 14','Maximum AS-path length 15',\n",
    "                  'Maximum AS-path length 16','Maximum AS-path length 17','Maximum AS-path length 18','Maximum AS-path length 19','Maximum AS-path length 20', \n",
    "                  'Maximum edit distance 7','Maximum edit distance 8','Maximum edit distance 9','Maximum edit distance 10','Maximum edit distance 11',\n",
    "                  'Maximum edit distance 12','Maximum edit distance 13','Maximum edit distance 14','Maximum edit distance 15','Maximum edit distance 16',\n",
    "                  'Number of Interior Gateway Protocol (IGP) packets', 'Number of Exterior Gateway Protocol (EGP) packets','Number of incomplete packets','Packet size (B)','regular or anomalous']\n",
    "    for i in range(len(new_col_name)):\n",
    "        file.rename(columns={col_name[i]:new_col_name[i]},inplace=True)\n",
    "    return file.iloc[:,4:42]\n",
    "\n",
    "#fisher_markov selector\n",
    "def fisher_markov_selector(dataset,w,N):\n",
    "    X=dataset.iloc[:,:-1]\n",
    "    y=dataset[\"regular or anomalous\"]\n",
    "    std=StandardScaler()\n",
    "    X_std=std.fit_transform(X)\n",
    "\n",
    "    std_dataset=pd.DataFrame(X_std,columns=X.columns)\n",
    "    std_dataset[\"regular or anomalous\"]=y.values\n",
    "\n",
    "    std_dataset.loc[std_dataset[\"regular or anomalous\"]==1,std_dataset.columns != \"regular or anomalous\"]*=w\n",
    "    \n",
    "    gamma=-0.5\n",
    "    n, p = X.shape\n",
    "\n",
    "    theta = np.zeros(p)\n",
    "\n",
    "    for j in range(p):\n",
    "        # Calculate the means and other statistics\n",
    "        theta_j = 0\n",
    "        for k in np.unique(y):\n",
    "            X_k = std_dataset[y == k]\n",
    "            n_k = len(X_k)\n",
    "            theta_j += (1/n) * np.sum(X_k.iloc[:, j]) * np.sum(X_k.iloc[:, j]) / n_k\n",
    "\n",
    "        theta_j -= gamma * np.sum(std_dataset.iloc[:, j])**2 / n\n",
    "        theta_j += (gamma - 1) / n**2 * np.sum(std_dataset.iloc[:, j])**2\n",
    "\n",
    "        # Set theta value\n",
    "        theta[j] = theta_j\n",
    "    idx=[]\n",
    "    theta_score=[]\n",
    "    # 피쳐 선택\n",
    "    if N==37:\n",
    "        idx=[i for i in range(37)]\n",
    "        return idx,theta\n",
    "    for i in range(N):\n",
    "        m_idx = np.argmax(theta)\n",
    "        idx.append(m_idx)\n",
    "        theta_score.append(theta[m_idx])\n",
    "        theta[m_idx] = -1000\n",
    "    return idx,theta_score\n",
    "\n",
    "data_list=[BCNET,RED1,Nimda,RIPE,Slammer]\n",
    "data_name=['BCNET','RED1','Nimda','RIPE','Slammer']\n",
    "\n",
    "#열이름을 붙여 데이터 생성\n",
    "BCNET=plus_col(BCNET) ; RED1=plus_col(RED1) ; Nimda=plus_col(Nimda) ;RIPE=plus_col(RIPE) ;Slammer=plus_col(Slammer)\n",
    "\n",
    "#데이터셋 생성\n",
    "dataset1=pd.concat([Slammer,Nimda],ignore_index=True)#2.88\n",
    "dataset2=pd.concat([Slammer,RED1],ignore_index=True)#8.79\n",
    "dataset3=pd.concat([Nimda,RED1],ignore_index=True)#2.49\n",
    "\n",
    "\n",
    "#선택된 Feature와 Score\n",
    "idx1,score1=fisher_markov_selector(dataset1,2.88,10)\n",
    "idx2,score2=fisher_markov_selector(dataset2,8.79,10)\n",
    "idx3,score3=fisher_markov_selector(dataset3,2.49,10)\n",
    "score_table=pd.DataFrame({'dataset1':idx1,'theta1':score1,\n",
    "              'dataset2':idx2,'theta2':score2,\n",
    "              'dataset3':idx3,'theta3':score3})\n",
    "\n",
    "\n",
    "#SVM 모델 생성\n",
    "def svm_model(train_set,test_set,w,n,best_params):\n",
    "    select_features, _ = fisher_markov_selector(train_set,w,n)\n",
    "    \n",
    "    X_tr=train_set.iloc[:,select_features]\n",
    "    X_train=StandardScaler().fit_transform(X_tr)\n",
    "    y_train=train_set[\"regular or anomalous\"]\n",
    "\n",
    "    X_te=test_set.iloc[:,select_features]\n",
    "    X_test=StandardScaler().fit_transform(X_te)\n",
    "    y_test=test_set[\"regular or anomalous\"]\n",
    "\n",
    "    \n",
    "    svm=SVC(**best_params)\n",
    "    \n",
    "    svm.fit(X_train,y_train)\n",
    "    y_pred=svm.predict(X_test)\n",
    "    ac=accuracy_score(y_test,y_pred)\n",
    "    f1=f1_score(y_test,y_pred)\n",
    "    report=classification_report(y_test,y_pred)\n",
    "    return ac,f1,report\n",
    "\n",
    "#grid_search 로 찾아낸 C,Gamma\n",
    "d1={'C': 0.03125, 'gamma': 0.0625, 'kernel': 'rbf'}\n",
    "d2={'C': 16, 'gamma': 0.0625, 'kernel': 'rbf'}\n",
    "d3={'C': 32, 'gamma': 0.0625, 'kernel': 'rbf'}\n",
    "\n",
    "dataset1_accuracy,dataset1_f1,_=svm_model(dataset1,RED1,2.88,10,d1);dataset1_all_accuracy,dataset1_all_f1,_=svm_model(dataset1,RED1,2.88,37,d1)\n",
    "dataset2_accurcay,dataset2_f1,_=svm_model(dataset2,Nimda,8.79,10,d2);dataset2_all_accuracy,dataset2_all_f1,_=svm_model(dataset2,Nimda,8.79,37,d2)\n",
    "dataset3_accuracy,dataset3_f1,_=svm_model(dataset3,Slammer,2.49,10,d3);dataset3_all_accuracy,dataset3_all_f1,_=svm_model(dataset3,Slammer,2.49,37,d3)\n",
    "df_SVM=pd.DataFrame({'Dataset':['dataset1','dataset1_all','dataset2','dataset2_all','dataset3','dataset3_all'],\n",
    "                 'Accuracy':[dataset1_accuracy,dataset1_all_accuracy,dataset2_accurcay,dataset2_all_accuracy,dataset3_accuracy,dataset3_all_accuracy],\n",
    "                 'F1 Score':[dataset1_f1,dataset1_all_f1,dataset2_f1,dataset2_all_f1,dataset3_f1,dataset3_all_f1]\n",
    "                 })\n",
    "df_SVM\n",
    "\n",
    "#의사결정트리 모델 생성\n",
    "def decision_tree_model(train_set,test_set,w,n):\n",
    "    select_features, _ = fisher_markov_selector(train_set, w, n)\n",
    "    \n",
    "    X_tr = train_set.iloc[:, select_features]\n",
    "    X_train = StandardScaler().fit_transform(X_tr)\n",
    "    y_train = train_set[\"regular or anomalous\"]\n",
    "\n",
    "    X_te = test_set.iloc[:, select_features]\n",
    "    X_test = StandardScaler().fit_transform(X_te)\n",
    "    y_test = test_set[\"regular or anomalous\"]\n",
    "    param_grid = {'max_depth': [3, 5, 7, 10], 'min_samples_split': [2, 5, 10]}\n",
    "    grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    dt = DecisionTreeClassifier(**best_params)\n",
    "    \n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred = dt.predict(X_test)\n",
    "\n",
    "    ac = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return ac, f1, report\n",
    "\n",
    "dataset1_accuracy,dataset1_f1,_=decision_tree_model(dataset1,RED1,2.88,10);dataset1_all_accuracy,dataset1_all_f1,_=decision_tree_model(dataset1,RED1,2.88,37)\n",
    "dataset2_accurcay,dataset2_f1,_=decision_tree_model(dataset2,Nimda,8.79,10);dataset2_all_accuracy,dataset2_all_f1,_=decision_tree_model(dataset2,Nimda,8.79,37)\n",
    "dataset3_accuracy,dataset3_f1,_=decision_tree_model(dataset3,Slammer,2.49,10);dataset3_all_accuracy,dataset3_all_f1,_=decision_tree_model(dataset3,Slammer,2.49,37)\n",
    "df_Trees=pd.DataFrame({'Dataset':['dataset1','dataset1_all','dataset2','dataset2_all','dataset3','dataset3_all'],\n",
    "                 'Accuracy':[dataset1_accuracy,dataset1_all_accuracy,dataset2_accurcay,dataset2_all_accuracy,dataset3_accuracy,dataset3_all_accuracy],\n",
    "                 'F1 Score':[dataset1_f1,dataset1_all_f1,dataset2_f1,dataset2_all_f1,dataset3_f1,dataset3_all_f1]\n",
    "                })\n",
    "df_Trees\n",
    "#iForest 모델생성\n",
    "def iForest(train_set,test_set,w,n):\n",
    "    select_features, _ = fisher_markov_selector(train_set,w,n)\n",
    "    \n",
    "    X_tr=train_set.iloc[:,select_features]\n",
    "    X_train=StandardScaler().fit_transform(X_tr)\n",
    "    y_train=train_set[\"regular or anomalous\"]\n",
    "\n",
    "    X_te=test_set.iloc[:,select_features]\n",
    "    X_test=StandardScaler().fit_transform(X_te)\n",
    "    y_test=test_set[\"regular or anomalous\"]\n",
    "\n",
    "    iforest=IsolationForest(n_estimators=100,contamination=0.01,random_state=42)\n",
    "    iforest.fit(X_train,y_train)\n",
    "    y_pred=iforest.predict(X_test)\n",
    "\n",
    "    ac=accuracy_score(y_test,y_pred)\n",
    "    f1=f1_score(y_test,y_pred)\n",
    "    report=classification_report(y_test,y_pred)\n",
    "    return ac,f1,report\n",
    "\n",
    "\n",
    "\n",
    "dataset1_accuracy,dataset1_f1,_=iForest(dataset1,RED1,2.88,10);dataset1_all_accuracy,dataset1_all_f1,_=iForest(dataset1,RED1,2.88,37)\n",
    "dataset2_accurcay,dataset2_f1,_=iForest(dataset2,Nimda,8.79,10);dataset2_all_accuracy,dataset2_all_f1,_=iForest(dataset2,Nimda,8.79,37)\n",
    "dataset3_accuracy,dataset3_f1,_=iForest(dataset3,Slammer,2.49,10);dataset3_all_accuracy,dataset3_all_f1,_=iForest(dataset3,Slammer,2.49,37)\n",
    "df_iForest=pd.DataFrame({'Dataset':['dataset1','dataset1_all','dataset2','dataset2_all','dataset3','dataset3_all'],\n",
    "                 'Accuracy':[dataset1_accuracy,dataset1_all_accuracy,dataset2_accurcay,dataset2_all_accuracy,dataset3_accuracy,dataset3_all_accuracy],\n",
    "                 'F1 Score':[dataset1_f1,dataset1_all_f1,dataset2_f1,dataset2_all_f1,dataset3_f1,dataset3_all_f1]\n",
    "                })\n",
    "df_iForest\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
